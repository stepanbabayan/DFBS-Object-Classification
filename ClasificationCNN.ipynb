{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, fnmatch\n",
    "from os import listdir, path as os_path\n",
    "import pickle\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from _helpers import make_directory\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from tensorflow import device as tf_device\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'Subtypes'\n",
    "setting = 'Lets combine!'\n",
    "main_path = f'data/Extracted/{csv_name}/{setting}/images_classified_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_TEST_SPLIT   (RUN ONLY IF images_classified_raw folder contains only the \"out\" folder)\n",
    "\n",
    "all_data_path = f'{main_path}/out'\n",
    "\n",
    "train_path = os_path.join(main_path, 'train')\n",
    "test_path = os_path.join(main_path, 'test')\n",
    "\n",
    "make_directory(train_path)\n",
    "make_directory(test_path)\n",
    "\n",
    "train_entries, test_entries = train_test_split(listdir(all_data_path), test_size=0.15, shuffle=True)\n",
    "\n",
    "for entry in train_entries:\n",
    "    entry_path = os_path.join(all_data_path, entry)\n",
    "    entry_output_path = os_path.join(train_path, entry)\n",
    "    shutil.copyfile(entry_path, entry_output_path)\n",
    "\n",
    "for entry in test_entries:\n",
    "    entry_path = os_path.join(all_data_path, entry)\n",
    "    entry_output_path = os_path.join(test_path, entry)\n",
    "    shutil.copyfile(entry_path, entry_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(main_path, 'train')\n",
    "data_train = []\n",
    "data_train_names = os.listdir(train_path)\n",
    "pattern = \"*.tiff\"\n",
    "for entry in data_train_names:\n",
    "    if fnmatch.fnmatch(entry, pattern):\n",
    "            data_train.append(os.path.join(train_path, entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1865 1865\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train_names), len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_path = os.path.join(main_path, 'test')\n",
    "data_test = []\n",
    "data_test_names = os.listdir(test_path)\n",
    "pattern = \"*.tiff\"\n",
    "for entry in data_test_names:\n",
    "    if fnmatch.fnmatch(entry, pattern):\n",
    "            data_test.append(os.path.join(test_path, entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 330\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test_names), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train_list = []\n",
    "for i in range(len(data_train)):\n",
    "    arr_obj = []\n",
    "    cl = (data_train[i].split(\"/\")[-1]).split(\"~~~\")[0]\n",
    "#     if cl == \"Sy1\" or cl == \"cv\" or cl == \"WD\" or cl == \"QSO\": continue\n",
    "    if cl not in {'sdB', 'C-H', 'Mrk SB', 'C Ba', 'sdO', 'sdA'}: continue\n",
    "    arr_obj.append(cl)\n",
    "    arr_obj.append(data_train[i])\n",
    "    data_train_list.append(arr_obj)\n",
    "    \n",
    "data_test_list = []\n",
    "for i in range(len(data_test)):\n",
    "    arr_obj = []\n",
    "    cl = (data_test[i].split(\"/\")[-1]).split(\"~~~\")[0]\n",
    "#     if cl == \"Sy1\" or cl == \"cv\" or cl == \"WD\" or cl == \"QSO\": continue\n",
    "    if cl not in {'sdB', 'C-H', 'Mrk SB', 'C Ba', 'sdO', 'sdA'}: continue\n",
    "    arr_obj.append(cl)\n",
    "    arr_obj.append(data_test[i])\n",
    "    data_test_list.append(arr_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668 297\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train_list), len(data_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data_train_list)\n",
    "df_train['fname'] = df_train[1].str.split('/', expand=True, ).iloc[:,-1]\n",
    "df_train.rename(columns={1: \"path\", 0: \"Cl\"}, inplace=True)\n",
    "df_test = pd.DataFrame(data_test_list)\n",
    "df_test['fname'] = df_test[1].str.split('/', expand=True, ).iloc[:,-1]\n",
    "df_test.rename(columns={1: \"path\", 0: \"Cl\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cl</th>\n",
       "      <th>path</th>\n",
       "      <th>fname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrk SB</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>Mrk SB~~~1285__354.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C Ba</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C Ba~~~22__J025658.37+332608.6.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C-H</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C-H~~~464__J132626.19+192957.1.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C-H</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C-H~~~574__J233418.64+200250.9.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sdO</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdO~~~3026__PG1204+543.tiff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cl                                               path  \\\n",
       "0  Mrk SB  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "1    C Ba  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "2     C-H  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "3     C-H  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "4     sdO  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "\n",
       "                                 fname  \n",
       "0              Mrk SB~~~1285__354.tiff  \n",
       "1  C Ba~~~22__J025658.37+332608.6.tiff  \n",
       "2  C-H~~~464__J132626.19+192957.1.tiff  \n",
       "3  C-H~~~574__J233418.64+200250.9.tiff  \n",
       "4          sdO~~~3026__PG1204+543.tiff  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cl</th>\n",
       "      <th>path</th>\n",
       "      <th>fname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sdA</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdA~~~1949__SDSSJ112504.73+671658.3.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C-H</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C-H~~~267__J144814.56+024352.7.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C Ba</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C Ba~~~240__J092132.77+072133.8.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sdO</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdO~~~2880__GALEXJ05557+6408.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrk SB</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>Mrk SB~~~1233__1208.tiff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cl                                               path  \\\n",
       "0     sdA  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "1     C-H  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "2    C Ba  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "3     sdO  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "4  Mrk SB  data/Extracted/Subtypes/Lets combine!/images_c...   \n",
       "\n",
       "                                      fname  \n",
       "0  sdA~~~1949__SDSSJ112504.73+671658.3.tiff  \n",
       "1       C-H~~~267__J144814.56+024352.7.tiff  \n",
       "2      C Ba~~~240__J092132.77+072133.8.tiff  \n",
       "3         sdO~~~2880__GALEXJ05557+6408.tiff  \n",
       "4                  Mrk SB~~~1233__1208.tiff  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_directory(path):\n",
    "    folders = path.split('/')\n",
    "    current_path = ''\n",
    "    for folder in folders[:-1]:\n",
    "        current_path += folder + '/' \n",
    "        try:\n",
    "            os.mkdir(current_path)\n",
    "        except OSError as error:\n",
    "            pass\n",
    "    current_path += folders[-1]\n",
    "    shutil.rmtree(current_path, ignore_errors=True)\n",
    "    os.mkdir(current_path)\n",
    "\n",
    "def augment(data, n_times=1, batch_size=2, img_size=(140,20), input_path='data/images/',\n",
    "            output_path='data/augmented/', seed = None, save_format='png', x_col='fname', y_col=\"Cl\",\n",
    "            shuffle=False, color_mode='grayscale', class_mode=\"categorical\"):\n",
    "    n_steps_data_aug = np.ceil(data.shape[0]/batch_size).astype(int)\n",
    "    save_prefix = 'aug'\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=1,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode=\"nearest\"\n",
    "        # rescale = 1./0xff\n",
    "    )\n",
    "    \n",
    "    aug_gen = datagen.flow_from_dataframe(dataframe=data, directory=input_path,\n",
    "                                          save_to_dir=output_path, save_prefix=save_prefix,\n",
    "                                          save_format=save_format, x_col=x_col, y_col=y_col,\n",
    "                                          batch_size=batch_size, seed=seed,\n",
    "                                          shuffle=shuffle, color_mode=color_mode,\n",
    "                                          class_mode=class_mode, target_size=img_size)\n",
    "\n",
    "    make_directory(output_path)\n",
    "\n",
    "    for i in tqdm(range(n_times*n_steps_data_aug)):\n",
    "        next(aug_gen)\n",
    "\n",
    "    augmented_images = np.array(os.listdir(output_path))\n",
    "    aug_data = pd.concat([pd.Series(augmented_images).str.split('_', expand=True)[1], output_path + pd.Series(augmented_images)], axis=1)\n",
    "\n",
    "    aug_data[y_col] = data[y_col].iloc[aug_data[1]].values\n",
    "    aug_data[1] = data.iloc[aug_data[1]].index\n",
    "    aug_data.rename(columns={0: \"path\", 1: \"data_index\"}, inplace=True)\n",
    "\n",
    "    return aug_data, aug_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img_size = (140, 20)\n",
    "train_out_path = os.path.join(main_path, 'augmented/train/')\n",
    "test_out_path = os.path.join(main_path, 'augmented/test/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1668 validated image filenames belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 3336/3336 [00:10<00:00, 315.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 297 validated image filenames belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 285.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_index</th>\n",
       "      <th>path</th>\n",
       "      <th>Cl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1039</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>Mrk SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C-H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>Mrk SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_index                                               path      Cl\n",
       "0         731  data/Extracted/Subtypes/Lets combine!/images_c...     sdA\n",
       "1        1039  data/Extracted/Subtypes/Lets combine!/images_c...  Mrk SB\n",
       "2         231  data/Extracted/Subtypes/Lets combine!/images_c...     C-H\n",
       "3         128  data/Extracted/Subtypes/Lets combine!/images_c...  Mrk SB\n",
       "4         410  data/Extracted/Subtypes/Lets combine!/images_c...     sdB"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_train, aug_classes = augment(df_train, n_times=8, batch_size=4, img_size=img_size,\n",
    "                                 input_path=train_path, output_path=train_out_path,\n",
    "                                 seed=None, save_format='tiff')\n",
    "aug_test, _ = augment(df_test, n_times=2, batch_size=4, img_size=img_size, input_path=test_path,\n",
    "                      output_path=test_out_path, seed=None, save_format='tiff')\n",
    "aug_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = aug_train\n",
    "df_test = aug_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_index</th>\n",
       "      <th>path</th>\n",
       "      <th>Cl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1039</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>Mrk SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>C-H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>Mrk SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410</td>\n",
       "      <td>data/Extracted/Subtypes/Lets combine!/images_c...</td>\n",
       "      <td>sdB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_index                                               path      Cl\n",
       "0         731  data/Extracted/Subtypes/Lets combine!/images_c...     sdA\n",
       "1        1039  data/Extracted/Subtypes/Lets combine!/images_c...  Mrk SB\n",
       "2         231  data/Extracted/Subtypes/Lets combine!/images_c...     C-H\n",
       "3         128  data/Extracted/Subtypes/Lets combine!/images_c...  Mrk SB\n",
       "4         410  data/Extracted/Subtypes/Lets combine!/images_c...     sdB"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdB       4272\n",
       "C-H       3232\n",
       "Mrk SB    3080\n",
       "C Ba      1024\n",
       "sdA        880\n",
       "sdO        856\n",
       "Name: Cl, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdB       160\n",
       "Mrk SB    148\n",
       "C-H       138\n",
       "C Ba       62\n",
       "sdO        52\n",
       "sdA        34\n",
       "Name: Cl, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train['Cl'])\n",
    "df_train['Cl']=le.transform(df_train['Cl'])\n",
    "df_test['Cl']=le.transform(df_test['Cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    4272\n",
       "1    3232\n",
       "2    3080\n",
       "0    1024\n",
       "3     880\n",
       "5     856\n",
       "Name: Cl, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    160\n",
       "2    148\n",
       "1    138\n",
       "0     62\n",
       "5     52\n",
       "3     34\n",
       "Name: Cl, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = df_train['path']\n",
    "Y_tr = df_train['Cl']\n",
    "X_tr = X_tr.values\n",
    "Y_tr = Y_tr.values\n",
    "\n",
    "X_ts = df_test['path']\n",
    "Y_ts = df_test['Cl']\n",
    "X_ts = X_ts.values\n",
    "Y_ts = Y_ts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_list_train = []\n",
    "max_width = 0\n",
    "max_height = 0\n",
    "ind = 0\n",
    "for i in range(len(X_tr)):\n",
    "    im = Image.open(X_tr[i])\n",
    "    arr = np.array(im)\n",
    "    \n",
    "    arr=(arr-arr.min())/(arr.max()-arr.min())\n",
    "\n",
    "    if arr.mean() > 0.5:\n",
    "        arr = 1 - arr\n",
    "\n",
    "    s = arr.shape\n",
    "    if s[0] > max_height:\n",
    "        max_height = s[0]\n",
    "    if s[1] > max_width:\n",
    "        max_width = s[1]\n",
    "        ind = i\n",
    "    images_list_train.append(arr)\n",
    "    \n",
    "images_list_test = []\n",
    "ind = 0\n",
    "for i in range(len(X_ts)):\n",
    "    im = Image.open(X_ts[i])\n",
    "    arr = np.array(im)\n",
    "    \n",
    "    arr=(arr-arr.min())/(arr.max()-arr.min())\n",
    "\n",
    "    if arr.mean() > 0.5:\n",
    "        arr = 1 - arr\n",
    "    \n",
    "    s = arr.shape\n",
    "    if s[0] > max_height:\n",
    "        max_height = s[0]\n",
    "    if s[1] > max_width:\n",
    "        max_width = s[1]\n",
    "        ind = i\n",
    "    images_list_test.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_height, max_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_width = 50\n",
    "max_height = 160\n",
    "max_width = 32\n",
    "max_height = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 144\n"
     ]
    }
   ],
   "source": [
    "print(max_width, max_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(images_list_train)):\n",
    "    s = images_list_train[i].shape\n",
    "    d_width = (max_width - s[1])\n",
    "    d_height = (max_height - s[0])\n",
    "    \n",
    "    d_top = int(d_height / 2)\n",
    "    d_bottom = int(d_height - d_top)\n",
    "    \n",
    "    d_left = int(d_width / 2)\n",
    "    d_right = int(d_width - d_left)\n",
    "    #print(d_top, d_bottom, d_left, d_right)\n",
    "    \n",
    "    arr = images_list_train[i]\n",
    "    for l in range(d_left):\n",
    "        arr = np.insert(arr, 0, 0, axis = 1)\n",
    "    \n",
    "    for r in range(d_right):\n",
    "        b = np.zeros((s[0],1))\n",
    "        arr = np.append(arr, b, axis = 1)\n",
    "    \n",
    "    for t in range(d_top):\n",
    "        arr = np.insert(arr, 0, 0, axis = 0)\n",
    "    \n",
    "    for b in range(d_bottom):\n",
    "        b = np.zeros((1, arr.shape[1],))\n",
    "        arr = np.append(arr, b, axis = 0)\n",
    "    \n",
    "    images_list_train[i] = arr.flatten()\n",
    "\n",
    "for i in range(len(images_list_test)):\n",
    "    s = images_list_test[i].shape\n",
    "    d_width = (max_width - s[1])\n",
    "    d_height = (max_height - s[0])\n",
    "    \n",
    "    d_top = int(d_height / 2)\n",
    "    d_bottom = int(d_height - d_top)\n",
    "    \n",
    "    d_left = int(d_width / 2)\n",
    "    d_right = int(d_width - d_left)\n",
    "    #print(d_top, d_bottom, d_left, d_right)\n",
    "    \n",
    "    arr = images_list_test[i]\n",
    "    for l in range(d_left):\n",
    "        arr = np.insert(arr, 0, 0, axis = 1)\n",
    "    \n",
    "    for r in range(d_right):\n",
    "        b = np.zeros((s[0],1))\n",
    "        arr = np.append(arr, b, axis = 1)\n",
    "    \n",
    "    for t in range(d_top):\n",
    "        arr = np.insert(arr, 0, 0, axis = 0)\n",
    "    \n",
    "    for b in range(d_bottom):\n",
    "        b = np.zeros((1, arr.shape[1],))\n",
    "        arr = np.append(arr, b, axis = 0)\n",
    "    \n",
    "    images_list_test[i] = arr.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_np_train = np.array(images_list_train)\n",
    "images_np_test = np.array(images_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_count = len(df_train['Cl'].unique())\n",
    "X_train = images_np_train\n",
    "Y_train = Y_tr\n",
    "Y_train = to_categorical(Y_train, class_count)\n",
    "\n",
    "X_test = images_np_test\n",
    "Y_test = Y_ts\n",
    "Y_test = to_categorical(Y_test, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], max_height, max_width, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], max_height, max_width, 1)\n",
    "input_shape = (max_height, max_width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (13344, 144, 32, 1)\n",
      "13344 train samples\n",
      "594 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = np.concatenate([X_train, X_train, X_train], axis=-1)\n",
    "X_test_3 = np.concatenate([X_test, X_test, X_test], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13344, 144, 32, 3), (594, 144, 32, 3), (144, 32, 3))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_3.shape, X_test_3.shape, (input_shape[0], input_shape[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 16:18:19.343723: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-09-24 16:18:19.343754: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: mlgod-GF63-Thin-11UC\n",
      "2022-09-24 16:18:19.343759: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: mlgod-GF63-Thin-11UC\n",
      "2022-09-24 16:18:19.343991: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.141.3\n",
      "2022-09-24 16:18:19.344006: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.141.3\n",
      "2022-09-24 16:18:19.344010: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.141.3\n",
      "2022-09-24 16:18:19.344703: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tl = DenseNet121(include_top=False, weights='imagenet', input_shape=(input_shape[0], input_shape[1], 3), pooling=None)\n",
    "# tl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in tl.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet121 (Functional)    (None, 4, 1, 1024)        7037504   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               524416    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,562,694\n",
      "Trainable params: 525,190\n",
      "Non-trainable params: 7,037,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modTL = Sequential()\n",
    "modTL.add(Input(shape=(input_shape[0], input_shape[1], 3)))\n",
    "# modTL.add(Conv2D(filters=3, kernel_size=(1,1), activation='relu', padding='same'))\n",
    "modTL.add(tl)\n",
    "modTL.add(Flatten())\n",
    "modTL.add(Dense(128, activation='relu'))\n",
    "modTL.add(Dropout(0.5))\n",
    "modTL.add(Dense(class_count,activation='softmax'))\n",
    "modTL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta = optimizers.Adadelta(learning_rate=0.01, rho=0.95)\n",
    "modTL.compile(optimizer='adamax' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 16:18:33.524390: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 737869824 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   25/13344 [..............................] - ETA: 3:57 - loss: 15.9540 - accuracy: 0.2000 "
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[0] = 5 is not in [0, 5)\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_19630]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# counter = Counter(aug_classes)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# max_val = float(max(counter.values()))       \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# cw = {class_id : max_val/num_images for class_id, num_images in counter.items()}\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf_device(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodTL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DFBS-Object-Classification/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/DFBS-Object-Classification/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[0] = 5 is not in [0, 5)\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_19630]"
     ]
    }
   ],
   "source": [
    "checkpoint_directory = f'data/Checkpoints/{csv_name}/{setting}/'\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_directory + 'checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "make_directory(checkpoint_directory)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.002\n",
    "    if epoch > 3:\n",
    "        lrate = 0.00001\n",
    "    if epoch > 6:\n",
    "        lrate = 0.000005\n",
    "    if epoch > 9:\n",
    "        lrate = 0.0000025\n",
    "    if epoch > 12:\n",
    "        lrate = 0.000001\n",
    "    if epoch > 15:\n",
    "        lrate = 0.0000005\n",
    "    return lrate\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "cw = {0:3, 1:4, 2:3, 3:4, 4:3}\n",
    "\n",
    "# counter = Counter(aug_classes)\n",
    "# max_val = float(max(counter.values()))       \n",
    "# cw = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "\n",
    "with tf_device('GPU:0'):\n",
    "    history = modTL.fit(X_train_3, Y_train, epochs = 10, batch_size = 1, shuffle=True, validation_data=(X_test_3, Y_test), class_weight=cw, callbacks=[model_checkpoint_callback])\n",
    "#     Learning rate scheduler\n",
    "#     lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf_device('GPU:0'):\n",
    "    history_TL1 = modTL.fit(X_train, Y_train, epochs = 10, batch_size = 1, shuffle=True, validation_data=(X_test, Y_test), class_weight=cw, callbacks=[model_checkpoint_callback, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_network = MobileNetV2(include_top=False, weights=None, input_shape=input_shape, pooling='max')\n",
    "tl_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EfB7 = EfficientNetB7(include_top=False, weights=None, input_shape=input_shape, pooling='max')\n",
    "# EfB7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modB7 = Sequential()\n",
    "modB7.add(Input(input_shape))\n",
    "modB7.add(tl_network)\n",
    "modB7.add(Dense(class_count,activation='softmax'))\n",
    "modB7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta = optimizers.Adadelta(learning_rate=0.01, rho=0.95)\n",
    "modB7.compile(optimizer='adamax' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = f'data/Checkpoints/{csv_name}/{setting}/'\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_directory + 'checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "make_directory(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = {0:3, 1:4, 2:3, 3:4, 4:3}\n",
    "\n",
    "# counter = Counter(aug_classes)\n",
    "# max_val = float(max(counter.values()))       \n",
    "# cw = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "\n",
    "with tf_device('GPU:0'):\n",
    "    history = modB7.fit(X_train, Y_train, epochs = 10, batch_size = 1, shuffle=True, validation_data=(X_test, Y_test), class_weight=cw, callbacks=[model_checkpoint_callback, lr_scheduler])\n",
    "#     Learning rate scheduler\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(3,3), input_shape=input_shape, padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(class_count,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 144, 32, 128)      1280      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 72, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 72, 16, 128)       147584    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 72, 16, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 72, 8, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 72, 8, 64)         73792     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 72, 8, 64)         0         \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 36, 8, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 36, 8, 64)         36928     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 36, 8, 64)         0         \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 18, 4, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 18, 4, 64)         36928     \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 18, 4, 64)         0         \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 9, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,479,750\n",
      "Trainable params: 1,479,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adadelta = optimizers.Adadelta(learning_rate=0.01, rho=0.95)\n",
    "model.compile(optimizer='adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = f'data/Checkpoints/{csv_name}/{setting}/'\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_directory + 'checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "make_directory(checkpoint_directory)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.002\n",
    "    if epoch > 3:\n",
    "        lrate = 0.001\n",
    "    if epoch > 6:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 9:\n",
    "        lrate = 0.00025\n",
    "    if epoch > 12:\n",
    "        lrate = 0.0001\n",
    "    if epoch > 15:\n",
    "        lrate = 0.00005\n",
    "    return lrate\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1668/1668 [==============================] - 120s 72ms/step - loss: 2.4508 - accuracy: 0.4156 - val_loss: 1.0464 - val_accuracy: 0.5741\n",
      "Epoch 2/35\n",
      "1668/1668 [==============================] - 120s 72ms/step - loss: 1.9383 - accuracy: 0.5186 - val_loss: 1.0530 - val_accuracy: 0.4832\n",
      "Epoch 3/35\n",
      "1668/1668 [==============================] - 119s 72ms/step - loss: 1.7306 - accuracy: 0.5579 - val_loss: 1.0474 - val_accuracy: 0.4983\n",
      "Epoch 4/35\n",
      "1668/1668 [==============================] - 124s 75ms/step - loss: 1.5914 - accuracy: 0.5879 - val_loss: 1.1084 - val_accuracy: 0.5034\n",
      "Epoch 5/35\n",
      "1668/1668 [==============================] - 116s 70ms/step - loss: 1.4672 - accuracy: 0.6109 - val_loss: 1.0755 - val_accuracy: 0.5438\n",
      "Epoch 6/35\n",
      "1100/1668 [==================>...........] - ETA: 38s - loss: 1.3008 - accuracy: 0.6492"
     ]
    }
   ],
   "source": [
    "#cw = {0:1, 1:1, 2:1, 3:1, 4:1}\n",
    "# cw = {0:3, 1:4, 2:3, 3:4, 4:3}\n",
    "\n",
    "counter = Counter(aug_classes)\n",
    "max_val = float(max(counter.values()))       \n",
    "cw = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "\n",
    "with tf_device('GPU:0'):\n",
    "    history = model.fit(X_train, Y_train, epochs = 35, batch_size = 8, shuffle=True, validation_data=(X_test, Y_test), class_weight=cw, callbacks=[model_checkpoint_callback])\n",
    "#     Learning rate scheduler\n",
    "#     history = model.fit(X_train, Y_train, epochs = 20, batch_size = 8, shuffle=True, validation_data=(X_test, Y_test), class_weight=cw, callbacks=[model_checkpoint_callback, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf_device('GPU:0'):\n",
    "    history_1 = model.fit(X_train, Y_train, epochs = 15, batch_size = 8, shuffle=True, validation_data=(X_test, Y_test), class_weight=cw, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Accuracy (train data)', linewidth=3, markersize=12)\n",
    "plt.plot(history.history['val_accuracy'], label='Accuracy (validation data)', linewidth=3, markersize=12)\n",
    "plt.plot(history.history['loss'], label='loss (train data)', linewidth=3, markersize=12)\n",
    "plt.plot(history.history['val_loss'], label='loss (validation data)', linewidth=3, markersize=12)\n",
    "plt.title('Loss and Accuracy')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test = np.argmax(Y_test, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(Y_test, to_categorical(np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_directory + 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test = np.argmax(Y_test, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(Y_test, to_categorical(np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/Existing/FinalDataset/train/'\n",
    "train_path = f'data/Extracted/{csv_name}/{setting}/images_classified_raw/train/'\n",
    "data_train = []\n",
    "data_train_names = os.listdir(train_path)\n",
    "pattern = \"*.tiff\"\n",
    "for entry in data_train_names:\n",
    "    if fnmatch.fnmatch(entry, pattern):\n",
    "            data_train.append(train_path+entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/Existing/FinalDataset/test/'\n",
    "test_path = f'data/Extracted/{csv_name}/{setting}/images_classified_raw/test/'\n",
    "data_test = []\n",
    "data_test_names = os.listdir(test_path)\n",
    "pattern = \"*.tiff\"\n",
    "for entry in data_test_names:\n",
    "    if fnmatch.fnmatch(entry, pattern):\n",
    "            data_test.append(test_path+entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_list = []\n",
    "for i in range(len(data_train)):\n",
    "    arr_obj = []\n",
    "    cl = (data_train[i].split(\"/\")[-1]).split(\"~~~\")[0]\n",
    "    if cl == \"Sy1\" or cl == \"cv\" or cl == \"WD\" or cl == \"QSO\": continue\n",
    "    arr_obj.append(cl)\n",
    "    arr_obj.append(data_train[i])\n",
    "    data_train_list.append(arr_obj)\n",
    "    \n",
    "data_test_list = []\n",
    "for i in range(len(data_test)):\n",
    "    arr_obj = []\n",
    "    cl = (data_test[i].split(\"/\")[-1]).split(\"~~~\")[0]\n",
    "    if cl == \"Sy1\" or cl == \"cv\" or cl == \"WD\" or cl == \"QSO\": continue\n",
    "    arr_obj.append(cl)\n",
    "    arr_obj.append(data_test[i])\n",
    "    data_test_list.append(arr_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data_train_list)\n",
    "train_df['fname'] = train_df[1].str.split('/', expand=True, ).iloc[:,-1]\n",
    "train_df.rename(columns={1: \"path\", 0: \"Cl\"}, inplace=True)\n",
    "test_df = pd.DataFrame(data_test_list)\n",
    "test_df['fname'] = test_df[1].str.split('/', expand=True, ).iloc[:,-1]\n",
    "test_df.rename(columns={1: \"path\", 0: \"Cl\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['fname'] = train_df['fname'].str.split('~~~', expand=True).iloc[:,-1]\n",
    "test_df['fname'] = test_df['fname'].str.split('~~~', expand=True).iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['name'] = train_df['fname'].str.split('__', expand=True).iloc[:,-1]\n",
    "test_df['name'] = test_df['fname'].str.split('__', expand=True).iloc[:,-1]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_path = f'data/Extracted/{csv_name}/{setting}/images/'\n",
    "extracted_train_path = 'data/Existing/FinalDataset/train/'\n",
    "extracted_test_path = 'data/Existing/FinalDataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = pd.Series(listdir(extracted_path))\n",
    "extracted_train = pd.Series(listdir(extracted_train_path))\n",
    "extracted_test = pd.Series(listdir(extracted_test_path))\n",
    "\n",
    "extracted.head()\n",
    "extracted_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_train.name = 'fullName'\n",
    "extracted_test.name = 'fullName'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_train = pd.concat([extracted_train, extracted_train.str.split('~~~', expand=True)], axis=1)\n",
    "extracted_test = pd.concat([extracted_test, extracted_test.str.split('~~~', expand=True)], axis=1)\n",
    "extracted_df = pd.concat([extracted_path + extracted, extracted], axis=1)\n",
    "extracted_df.head()\n",
    "extracted_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.rename(columns={0:'path', 1:'fname'}, inplace=True)\n",
    "extracted_train.rename(columns={0:'Cl', 1:'name'}, inplace=True)\n",
    "extracted_test.rename(columns={0:'Cl', 1:'name'}, inplace=True)\n",
    "extracted_df.head()\n",
    "extracted_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_name = extracted_df['fname'].str.split('__', expand=True).values\n",
    "extracted_df['Cl'], extracted_df['name'] = cl_name[:,0], cl_name[:,1]\n",
    "\n",
    "extracted_train['path'] = extracted_train_path + extracted_train['fullName']\n",
    "extracted_test['path'] = extracted_test_path + extracted_test['fullName']\n",
    "\n",
    "extracted_df.head()\n",
    "extracted_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Datasets/Combined.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df['Cl'] = data.iloc[extracted_df['Cl']]['Cl'].values\n",
    "extracted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df['Cl'].value_counts()\n",
    "extracted_train['Cl'].value_counts(), extracted_test['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.drop(extracted_df[(extracted_df['Cl'] == 'cv') | (extracted_df['Cl'] == 'QSO') | (extracted_df['Cl'] == 'WD') | (extracted_df['Cl'] == 'Sy1')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.head()\n",
    "extracted_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ext = []\n",
    "test_ext = []\n",
    "ext = []\n",
    "\n",
    "for row, value in extracted_df.iterrows():\n",
    "    rows = train_df[train_df['fname'] == value['name']]\n",
    "    if rows.shape[0]:\n",
    "        train_ext.append((value['path'], value['name'], value['Cl']))\n",
    "        continue\n",
    "    rows = test_df[test_df['fname'] == value['name']]\n",
    "    if rows.shape[0]:\n",
    "        test_ext.append((value['path'], value['name'], value['Cl']))\n",
    "        continue\n",
    "    ext.append((value['path'], value['name'], value['Cl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "unseen = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, value in extracted_train.iterrows():\n",
    "    rows = train_df[train_df['name'] == value['name']]\n",
    "    if rows.shape[0]:\n",
    "        train.append((value['path'], value['fullName'], value['Cl']))\n",
    "        continue\n",
    "    rows = test_df[test_df['name'] == value['name']]\n",
    "    if rows.shape[0]:\n",
    "        test.append((value['path'], value['fullName'], value['Cl']))\n",
    "        continue\n",
    "    unseen.append((value['path'], value['fullName'], value['Cl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, value in extracted_test.iterrows():\n",
    "    rows = train_df[train_df['name'] == value['name']]\n",
    "    if rows.shape[0]:\n",
    "        train.append((value['path'], value['fullName'], value['Cl']))\n",
    "        continue\n",
    "    rows = test_df[test_df['name'] == value['name']]\n",
    "    if rows.shape[0]:\n",
    "        test.append((value['path'], value['fullName'], value['Cl']))\n",
    "        continue\n",
    "    unseen.append((value['path'], value['fullName'], value['Cl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ext), len(test_ext), len(ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test), len(unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ext = pd.DataFrame(train_ext, columns=['path', 'name', 'Cl'])\n",
    "test_ext = pd.DataFrame(test_ext, columns=['path', 'name', 'Cl'])\n",
    "ext = pd.DataFrame(ext, columns=['path', 'name', 'Cl'])\n",
    "\n",
    "train = pd.DataFrame(train, columns=['path', 'name', 'Cl'])\n",
    "test = pd.DataFrame(test, columns=['path', 'name', 'Cl'])\n",
    "unseen = pd.DataFrame(unseen, columns=['path', 'name', 'Cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ext.head()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cl'].value_counts(), test['Cl'].value_counts(), unseen['Cl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(train_ext['Cl'])\n",
    "# train_ext['Cl']=le.transform(train_ext['Cl'])\n",
    "# test_ext['Cl']=le.transform(test_ext['Cl'])\n",
    "# ext['Cl'] = le.transform(ext['Cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train['Cl'])\n",
    "train['Cl']=le.transform(train['Cl'])\n",
    "test['Cl']=le.transform(test['Cl'])\n",
    "unseen['Cl'] = le.transform(unseen['Cl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr = train_ext['path']\n",
    "# Y_tr = train_ext['Cl']\n",
    "# X_tr = X_tr.values\n",
    "# Y_tr = Y_tr.values\n",
    "\n",
    "# X_ts = test_ext['path']\n",
    "# Y_ts = test_ext['Cl']\n",
    "# X_ts = X_ts.values\n",
    "# Y_ts = Y_ts.values\n",
    "\n",
    "# X_e = ext['path']\n",
    "# Y_e = ext['Cl']\n",
    "# X_e = X_e.values\n",
    "# Y_e = Y_e.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = train['path']\n",
    "Y_tr = train['Cl']\n",
    "X_tr = X_tr.values\n",
    "Y_tr = Y_tr.values\n",
    "\n",
    "X_ts = test['path']\n",
    "Y_ts = test['Cl']\n",
    "X_ts = X_ts.values\n",
    "Y_ts = Y_ts.values\n",
    "\n",
    "X_e = unseen['path']\n",
    "Y_e = unseen['Cl']\n",
    "X_e = X_e.values\n",
    "Y_e = Y_e.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_train = []\n",
    "max_width = 0\n",
    "max_height = 0\n",
    "ind = 0\n",
    "for i in range(len(X_tr)):\n",
    "    im = Image.open(X_tr[i])\n",
    "    arr = np.array(im)\n",
    "    arr = np.flipud(arr)\n",
    "    \n",
    "    arr=(arr-arr.min())/(arr.max()-arr.min())\n",
    "\n",
    "    if arr.mean() > 0.5:\n",
    "        arr = 1 - arr\n",
    "\n",
    "    s = arr.shape\n",
    "    if s[0] > max_height:\n",
    "        max_height = s[0]\n",
    "    if s[1] > max_width:\n",
    "        max_width = s[1]\n",
    "        ind = i\n",
    "    il_train.append(arr)\n",
    "    \n",
    "il_test = []\n",
    "ind = 0\n",
    "for i in range(len(X_ts)):\n",
    "    im = Image.open(X_ts[i])\n",
    "    arr = np.array(im)\n",
    "    arr = np.flipud(arr)\n",
    "    \n",
    "    arr=(arr-arr.min())/(arr.max()-arr.min())\n",
    "\n",
    "    if arr.mean() > 0.5:\n",
    "        arr = 1 - arr\n",
    "\n",
    "    s = arr.shape\n",
    "    if s[0] > max_height:\n",
    "        max_height = s[0]\n",
    "    if s[1] > max_width:\n",
    "        max_width = s[1]\n",
    "        ind = i\n",
    "    il_test.append(arr)\n",
    "    \n",
    "il_e = []\n",
    "ind = 0\n",
    "for i in range(len(X_e)):\n",
    "    im = Image.open(X_e[i])\n",
    "    arr = np.array(im)\n",
    "    arr = np.flipud(arr)\n",
    "    \n",
    "    arr=(arr-arr.min())/(arr.max()-arr.min())\n",
    "\n",
    "    if arr.mean() > 0.5:\n",
    "        arr = 1 - arr\n",
    "\n",
    "    s = arr.shape\n",
    "    if s[0] > max_height:\n",
    "        max_height = s[0]\n",
    "    if s[1] > max_width:\n",
    "        max_width = s[1]\n",
    "        ind = i\n",
    "    il_e.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width = 50\n",
    "max_height = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(il_train)):\n",
    "    s = il_train[i].shape\n",
    "    d_width = (max_width - s[1])\n",
    "    d_height = (max_height - s[0])\n",
    "    \n",
    "    d_top = int(d_height / 2)\n",
    "    d_bottom = int(d_height - d_top)\n",
    "    \n",
    "    d_left = int(d_width / 2)\n",
    "    d_right = int(d_width - d_left)\n",
    "    \n",
    "    arr = il_train[i]\n",
    "    for l in range(d_left):\n",
    "        arr = np.insert(arr, 0, 0, axis = 1)\n",
    "    \n",
    "    for r in range(d_right):\n",
    "        b = np.zeros((s[0],1))\n",
    "        arr = np.append(arr, b, axis = 1)\n",
    "    \n",
    "    for t in range(d_top):\n",
    "        arr = np.insert(arr, 0, 0, axis = 0)\n",
    "    \n",
    "    for b in range(d_bottom):\n",
    "        b = np.zeros((1, arr.shape[1],))\n",
    "        arr = np.append(arr, b, axis = 0)\n",
    "    \n",
    "    il_train[i] = arr.flatten()\n",
    "\n",
    "for i in range(len(il_test)):\n",
    "    s = il_test[i].shape\n",
    "    d_width = (max_width - s[1])\n",
    "    d_height = (max_height - s[0])\n",
    "    \n",
    "    d_top = int(d_height / 2)\n",
    "    d_bottom = int(d_height - d_top)\n",
    "    \n",
    "    d_left = int(d_width / 2)\n",
    "    d_right = int(d_width - d_left)\n",
    "    \n",
    "    arr = il_test[i]\n",
    "    for l in range(d_left):\n",
    "        arr = np.insert(arr, 0, 0, axis = 1)\n",
    "\n",
    "    for r in range(d_right):\n",
    "        b = np.zeros((s[0],1))\n",
    "        arr = np.append(arr, b, axis = 1)\n",
    "\n",
    "    for t in range(d_top):\n",
    "        arr = np.insert(arr, 0, 0, axis = 0)\n",
    "\n",
    "    for b in range(d_bottom):\n",
    "        b = np.zeros((1, arr.shape[1],))\n",
    "        arr = np.append(arr, b, axis = 0)\n",
    "\n",
    "    il_test[i] = arr.flatten()\n",
    "\n",
    "for i in range(len(il_e)):\n",
    "    s = il_e[i].shape\n",
    "    d_width = (max_width - s[1])\n",
    "    d_height = (max_height - s[0])\n",
    "    \n",
    "    d_top = int(d_height / 2)\n",
    "    d_bottom = int(d_height - d_top)\n",
    "    \n",
    "    d_left = int(d_width / 2)\n",
    "    d_right = int(d_width - d_left)\n",
    "    \n",
    "    arr = il_e[i]\n",
    "    for l in range(d_left):\n",
    "        arr = np.insert(arr, 0, 0, axis = 1)\n",
    "    \n",
    "    for r in range(d_right):\n",
    "        b = np.zeros((s[0],1))\n",
    "        arr = np.append(arr, b, axis = 1)\n",
    "    \n",
    "    for t in range(d_top):\n",
    "        arr = np.insert(arr, 0, 0, axis = 0)\n",
    "    \n",
    "    for b in range(d_bottom):\n",
    "        b = np.zeros((1, arr.shape[1],))\n",
    "        arr = np.append(arr, b, axis = 0)\n",
    "    \n",
    "    il_e[i] = arr.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_train = np.array(il_train)\n",
    "inp_test = np.array(il_test)\n",
    "inp_e = np.array(il_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = 5\n",
    "X_train1 = inp_train\n",
    "Y_train1 = Y_tr\n",
    "Y_train1 = to_categorical(Y_train1, class_count)\n",
    "\n",
    "X_test1 = inp_test\n",
    "Y_test1 = Y_ts\n",
    "Y_test1 = to_categorical(Y_test1, class_count)\n",
    "\n",
    "X_e1 = inp_e\n",
    "Y_e1 = Y_e\n",
    "Y_e1 = to_categorical(Y_e1, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train1.reshape(X_train1.shape[0], max_height, max_width, 1)\n",
    "X_test1 = X_test1.reshape(X_test1.shape[0], max_height, max_width, 1)\n",
    "X_e1 = X_e1.reshape(X_e1.shape[0], max_height, max_width, 1)\n",
    "input_shape = (max_height, max_width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape:', X_train1.shape)\n",
    "print(X_train1.shape[0], 'train samples')\n",
    "print(X_test1.shape[0], 'test samples')\n",
    "print(X_e1.shape[0], 'ext samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train1, Y_train1, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test1, Y_test1, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_e1, Y_e1, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.figure(figsize=(4,8))\n",
    "    plt.imshow(X_e1[i]); plt.gray();\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
